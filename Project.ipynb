{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UZAOwHKFwom8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import json\n",
        "import torch\n",
        "import logging\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "from torch.cuda import Event\n",
        "from typing import List, Dict, Tuple\n",
        "from datetime import datetime\n",
        "import torch.distributed as dist\n",
        "from xgboost import XGBClassifier\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup, get_scheduler, AutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score, precision_score, recall_score\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import joblib\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Poz9sAar6nFz",
        "outputId": "e999b8c2-795e-4611-80cf-fd00ef4266a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(48915, 11)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/University/4-2/정보기술학회/data/medical_data.csv\", encoding = 'utf-8')\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YQlED7xktI9-"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# 데이터 전처리 및 준비\n",
        "def preprocess_data(data):\n",
        "    data.dropna(subset=['증상', '진료과목코드', '주상병코드'], inplace=True)\n",
        "    return data\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # 소문자 변환\n",
        "    text = re.sub(r\"[^가-힣a-zA-Z0-9\\\\s]\", \"\", text)  # 특수문자 제거\n",
        "    text = re.sub(r\"\\\\s+\", \" \", text).strip()  # 공백 정리\n",
        "    return text\n",
        "\n",
        "# Custom Dataset 정의\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        tokens = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=self.max_length\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': tokens['input_ids'].squeeze(0),\n",
        "            'attention_mask': tokens['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# KM-BERT 임베딩 함수 (Batch 처리)\n",
        "def get_embeddings_with_dataset(dataset, model, batch_size=64, num_workers=4):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "    embeddings = []\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Generating embeddings\"):\n",
        "        input_ids = batch[\"input_ids\"].squeeze(1).to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].squeeze(1).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :].cpu().numpy()\n",
        "        embeddings.append(output)\n",
        "\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "# 모델 학습 및 평가 클래스 정의\n",
        "class ModelTrainer:\n",
        "    def __init__(self, model, train_loader, val_loader, test_loader, device, num_classes, num_epochs=10):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "        # Optimizer and Scheduler\n",
        "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "        num_training_steps = len(train_loader) * self.num_epochs\n",
        "        num_warmup_steps = num_training_steps // 10\n",
        "        self.scheduler = get_scheduler(\n",
        "            \"linear\",\n",
        "            optimizer=self.optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            num_training_steps=num_training_steps\n",
        "        )\n",
        "\n",
        "        # Loss function\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train_epoch(self):\n",
        "        \"\"\"한 에폭의 학습을 수행하는 메서드\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        progress_bar = tqdm(self.train_loader, desc=\"Training\")\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(self.device)\n",
        "            attention_mask = batch['attention_mask'].to(self.device)\n",
        "            labels = batch['label'].to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            loss = self.criterion(logits, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        epoch_loss = total_loss / len(self.train_loader)\n",
        "        epoch_accuracy = accuracy_score(all_labels, all_preds)\n",
        "        epoch_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "        return {\n",
        "            'loss': epoch_loss,\n",
        "            'accuracy': epoch_accuracy,\n",
        "            'f1': epoch_f1\n",
        "        }\n",
        "\n",
        "    def evaluate(self, dataloader, mode='val'):\n",
        "        \"\"\"Validation 또는 Test 평가 메서드\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(dataloader, desc=f\"Evaluating ({mode})\"):\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                labels = batch['label'].to(self.device)\n",
        "\n",
        "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs.logits\n",
        "                loss = self.criterion(logits, labels)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "        metrics = {\n",
        "            f'{mode}_loss': avg_loss,\n",
        "            f'{mode}_accuracy': accuracy,\n",
        "            f'{mode}_f1': f1\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"전체 학습 수행\"\"\"\n",
        "        for epoch in range(self.num_epochs):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{self.num_epochs}\")\n",
        "\n",
        "            # Training\n",
        "            train_metrics = self.train_epoch()\n",
        "            print(f\"Training metrics: {train_metrics}\")\n",
        "\n",
        "            # Validation\n",
        "            val_metrics = self.evaluate(self.val_loader, mode='val')\n",
        "            print(f\"Validation metrics: {val_metrics}\")\n",
        "\n",
        "        # 최종 Test 평가\n",
        "        test_metrics = self.evaluate(self.test_loader, mode='test')\n",
        "        print(\"\\nFinal Test Results:\", test_metrics)\n",
        "\n",
        "        return test_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5EkZ-yOnM7_Y"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "\n",
        "data = preprocess_data(df)\n",
        "\n",
        "# Train-Test Split\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# 라벨 인코딩\n",
        "label_encoder_diagnosis = LabelEncoder()\n",
        "label_encoder_code = LabelEncoder()\n",
        "train_data['진료과목코드'] = label_encoder_diagnosis.fit_transform(train_data['진료과목코드'])\n",
        "test_data['진료과목코드'] = label_encoder_diagnosis.transform(test_data['진료과목코드'])\n",
        "train_data['주상병코드'] = label_encoder_code.fit_transform(train_data['주상병코드'])\n",
        "test_data['주상병코드'] = label_encoder_code.transform(test_data['주상병코드'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Wq7b1yvUCsNB"
      },
      "outputs": [],
      "source": [
        "#gpu 초기화\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAbyWCuCIxo7",
        "outputId": "68ea9af0-399a-4e5f-b64e-662533c6278a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at madatnlp/km-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KM-BERT Input IDs shape: torch.Size([64, 512])\n",
            "KM-BERT Attention Mask shape: torch.Size([64, 512])\n",
            "KM-BERT Labels shape: torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "# KM-BERT 모델 및 토크나이저 준비\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-BERT-char16424\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"madatnlp/km-bert\", num_labels=len(label_encoder_diagnosis.classes_))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 텍스트 데이터셋 및 데이터로더\n",
        "train_dataset = TextDataset(train_data['증상'].tolist(), train_data['진료과목코드'].tolist(), tokenizer)\n",
        "test_dataset = TextDataset(test_data['증상'].tolist(), test_data['진료과목코드'].tolist(), tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# 데이터 확인\n",
        "for batch in train_dataloader:\n",
        "    input_ids = batch['input_ids']\n",
        "    attention_mask = batch['attention_mask']\n",
        "    labels = batch['label']\n",
        "    print(f\"KM-BERT Input IDs shape: {input_ids.shape}\")\n",
        "    print(f\"KM-BERT Attention Mask shape: {attention_mask.shape}\")\n",
        "    print(f\"KM-BERT Labels shape: {labels.shape}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0l_bIzNI2Bz",
        "outputId": "a1496bdd-1969-4247-d641-59c9d29825a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 612/612 [12:00<00:00,  1.18s/it, loss=2.2825]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training metrics: {'loss': 2.2933085967902263, 'accuracy': 0.26773484616170906, 'f1': 0.22950174206191673}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating (val): 100%|██████████| 153/153 [01:01<00:00,  2.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation metrics: {'val_loss': 2.1887322812298544, 'val_accuracy': 0.29479709700500867, 'val_f1': 0.23644439492580255}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating (test): 100%|██████████| 153/153 [01:01<00:00,  2.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Test Results: {'test_loss': 2.1887322812298544, 'test_accuracy': 0.29479709700500867, 'test_f1': 0.23644439492580255}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'test_loss': 2.1887322812298544,\n",
              " 'test_accuracy': 0.29479709700500867,\n",
              " 'test_f1': 0.23644439492580255}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ModelTrainer 초기화 및 학습\n",
        "trainer = ModelTrainer(\n",
        "    model=model,\n",
        "    train_loader=train_dataloader,\n",
        "    val_loader=test_dataloader,\n",
        "    test_loader=test_dataloader,\n",
        "    device=device,\n",
        "    num_classes=len(label_encoder_diagnosis.classes_),\n",
        "    num_epochs=1\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOy1mJPaM-ec",
        "outputId": "bf87629a-4947-4b3d-acd4-e8c88bffb516"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting with KM-BERT: 100%|██████████| 612/612 [04:05<00:00,  2.49it/s]\n",
            "Predicting with KM-BERT: 100%|██████████| 153/153 [01:01<00:00,  2.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KM-BERT Train Probs Shape: (39132, 18)\n",
            "KM-BERT Test Probs Shape: (9783, 18)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# KM-BERT 전체 데이터셋 예측\n",
        "def predict_kmbert(model, dataloader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Predicting with KM-BERT\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "            predictions.append(probs)\n",
        "    return np.vstack(predictions)\n",
        "\n",
        "kmbert_train_probs = predict_kmbert(model, train_dataloader, device)\n",
        "kmbert_test_probs = predict_kmbert(model, test_dataloader, device)\n",
        "\n",
        "# KM-BERT 출력 크기 확인\n",
        "print(f\"KM-BERT Train Probs Shape: {kmbert_train_probs.shape}\")\n",
        "print(f\"KM-BERT Test Probs Shape: {kmbert_test_probs.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJYn8CNhNCJZ",
        "outputId": "3be43b8e-0f43-4639-aca8-cd09f18b99a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost Train Probs Shape: (39132, 18)\n",
            "XGBoost Test Probs Shape: (9783, 18)\n"
          ]
        }
      ],
      "source": [
        "# 1차 분류: XGBoost 학습\n",
        "# 정형 데이터 준비\n",
        "X_train_tabular = train_data[['성별코드', '연령대코드', '요양일수', '입내원일수', '총처방일수']]\n",
        "X_test_tabular = test_data[['성별코드', '연령대코드', '요양일수', '입내원일수', '총처방일수']]\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_tabular = scaler.fit_transform(one_hot_encoder.fit_transform(X_train_tabular))\n",
        "X_test_tabular = scaler.transform(one_hot_encoder.transform(X_test_tabular))\n",
        "\n",
        "# 1차 분류: XGBoost\n",
        "xgb_model = XGBClassifier()\n",
        "xgb_model.fit(X_train_tabular, train_data['진료과목코드'])\n",
        "xgb_train_probs = xgb_model.predict_proba(X_train_tabular)\n",
        "xgb_test_probs = xgb_model.predict_proba(X_test_tabular)\n",
        "\n",
        "# XGBoost 출력 크기 확인\n",
        "print(f\"XGBoost Train Probs Shape: {xgb_train_probs.shape}\")\n",
        "print(f\"XGBoost Test Probs Shape: {xgb_test_probs.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3lfV8s_NGPu",
        "outputId": "a4a38fac-d3d7-4fb8-ade6-e8706199035b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stack Train Input Shape: (39132, 36)\n",
            "Stack Test Input Shape: (9783, 36)\n",
            "1차 분류 Accuracy: 0.2540\n"
          ]
        }
      ],
      "source": [
        "# 1차 분류: Stack Ensemble\n",
        "stack_train_input = np.hstack([kmbert_train_probs, xgb_train_probs])\n",
        "stack_test_input = np.hstack([kmbert_test_probs, xgb_test_probs])\n",
        "\n",
        "# Stack Ensemble 입력 크기 확인\n",
        "print(f\"Stack Train Input Shape: {stack_train_input.shape}\")\n",
        "print(f\"Stack Test Input Shape: {stack_test_input.shape}\")\n",
        "\n",
        "stack_model = XGBClassifier()\n",
        "stack_model.fit(stack_train_input, train_data['진료과목코드'])\n",
        "stack_preds = stack_model.predict(stack_test_input)\n",
        "\n",
        "# 1차 분류 성능 평가\n",
        "print(f\"1차 분류 Accuracy: {accuracy_score(test_data['진료과목코드'], stack_preds):.4f}\")\n",
        "# print(\"Classification Report for 1차 분류:\")\n",
        "# print(classification_report(test_data['진료과목코드'], stack_preds, target_names=label_encoder_diagnosis.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HdMIyIZNEhN",
        "outputId": "5f97d622-7419-4479-e753-81672223a55f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2차 분류 Accuracy: 0.3609\n"
          ]
        }
      ],
      "source": [
        "# 2차 분류: 주상병코드 예측\n",
        "stack_input_2_train = np.hstack([stack_train_input, train_data['진료과목코드'].values.reshape(-1, 1)])\n",
        "stack_input_2_test = np.hstack([stack_test_input, test_data['진료과목코드'].values.reshape(-1, 1)])\n",
        "\n",
        "second_model = XGBClassifier()\n",
        "second_model.fit(stack_input_2_train, train_data['주상병코드'])\n",
        "y_second_pred = second_model.predict(stack_input_2_test)\n",
        "\n",
        "print(f\"2차 분류 Accuracy: {accuracy_score(test_data['주상병코드'], y_second_pred):.4f}\")\n",
        "# print(\"Classification Report for 2차 분류:\")\n",
        "# print(classification_report(test_data['주상병코드'], y_second_pred, target_names=label_encoder_code.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU7XEoOpT3Qi",
        "outputId": "77623c08-d6ac-4b19-9cb2-4114b760769a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "모델 및 인코더 저장 완료.\n"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/University/4-2/정보기술학회/data/model_filekmbert_finetuned_model.pt\")\n",
        "\n",
        "# 스택 모델 및 2차 모델 저장\n",
        "joblib.dump(stack_model, \"/content/drive/MyDrive/University/4-2/정보기술학회/data/model_filestack_model_1.pkl\")\n",
        "joblib.dump(second_model, \"/content/drive/MyDrive/University/4-2/정보기술학회/data/model_filestack_model_2.pkl\")\n",
        "\n",
        "# XGB 모델 저장 (1차 분류용)\n",
        "joblib.dump(xgb_model, \"/content/drive/MyDrive/University/4-2/정보기술학회/data/model_filexgb_model_for_1st_stage.pkl\")\n",
        "\n",
        "# 인코더 및 스케일러 저장\n",
        "joblib.dump(label_encoder_diagnosis, \"/content/drive/MyDrive/University/4-2/정보기술학회/data/model_filelabel_encoder_diagnosis.pkl\")\n",
        "joblib.dump(label_encoder_code, \"/content/drive/MyDrive/University/4-2/정보기술학회/data/model_filelabel_encoder_code.pkl\")\n",
        "joblib.dump(one_hot_encoder, \"/content/drive/MyDrive/University/4-2/정보기술학회/data/model_fileonehot_encoder.pkl\")\n",
        "joblib.dump(scaler, \"/content/drive/MyDrive/University/4-2/정보기술학회/data/model_filescaler.pkl\")\n",
        "\n",
        "print(\"모델 및 인코더 저장 완료.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
